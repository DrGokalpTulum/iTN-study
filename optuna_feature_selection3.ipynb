{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89042b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0) Imports & Global Configuration\n",
    "# ===============================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "import optuna\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ------------------ Global constants ------------------\n",
    "RANDOM_BASE = 42\n",
    "N_REPS = 20\n",
    "\n",
    "# 65/15/20 global split -> inner (train,val) over trainval(80%)\n",
    "TEST_FRACTION = 0.20\n",
    "VAL_FRACTION_GLOBAL = 0.15\n",
    "val_in_trainval   = VAL_FRACTION_GLOBAL / (1.0 - TEST_FRACTION)   # 0.1875\n",
    "train_in_trainval = 1.0 - val_in_trainval                         # 0.8125\n",
    "\n",
    "# Feature selection bounds (k-Best)\n",
    "K_MIN = 5\n",
    "K_MAX = 12\n",
    "\n",
    "# Optuna target: \"ap\" (PR-AUC), \"f1\", or \"auc\"\n",
    "OPTIMIZE_FOR = \"ap\"   # <<< set \"ap\" as requested (can be \"f1\"/\"auc\")\n",
    "\n",
    "# Optuna trials per model\n",
    "N_TRIALS_BY_MODEL = {\n",
    "    \"Random Forest\": 150,\n",
    "    \"SVM\":           150,\n",
    "    \"MLP\":           150,\n",
    "    \"XGBoost\":       150,\n",
    "    \"KNN\":           150,\n",
    "    \"Bagging\":       150,\n",
    "}\n",
    "\n",
    "# Paths (set to your environment)\n",
    "BASE_OUT_DIR = Path(r\"C:\\Users\\gokalp\\Desktop\\TN METRIC CALISMAnew\\31102025\\with_feature_selection2\")\n",
    "RUN_TAG      = f\"OBJ_{OPTIMIZE_FOR}\"\n",
    "MODEL_DIR    = BASE_OUT_DIR / f\"models_{RUN_TAG}\"\n",
    "OUT_DIR      = BASE_OUT_DIR / f\"outputs_{RUN_TAG}\"\n",
    "EXPL_DIR     = BASE_OUT_DIR / f\"explanations_plots_{RUN_TAG}\"\n",
    "LIME_DIR     = EXPL_DIR / \"lime_html\"\n",
    "STATS_PLOTS  = BASE_OUT_DIR / f\"stats_plots_{RUN_TAG}\"\n",
    "for d in [BASE_OUT_DIR, MODEL_DIR, OUT_DIR, EXPL_DIR, LIME_DIR, STATS_PLOTS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data path\n",
    "DATA_XLS_PATH = r\"C:\\Users\\gokalp\\Desktop\\TN METRIC CALISMAnew\\DS1\\SUITABLE\\TN Morfolojik_ALL_eng.xlsx\"\n",
    "\n",
    "SAVE_MODELS = True\n",
    "DO_EDA_PLOTS = True\n",
    "DO_EXPLAINERS = True\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 1) Load Data\n",
    "# ===============================================================\n",
    "data = pd.read_excel(DATA_XLS_PATH)\n",
    "if \"CLASS\" not in data.columns:\n",
    "    raise ValueError(\"Expected binary target column named 'CLASS' in the dataset.\")\n",
    "\n",
    "y = data[\"CLASS\"].values\n",
    "X = data.drop(columns=[\"CLASS\"])\n",
    "\n",
    "classes_sorted = np.unique(y)\n",
    "if len(classes_sorted) != 2:\n",
    "    raise ValueError(f\"Binary classification expected; got {len(classes_sorted)} classes.\")\n",
    "neg_label, pos_label = classes_sorted[0], classes_sorted[1]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2) Preprocessing (leakage-free in Pipeline)\n",
    "# ===============================================================\n",
    "def build_preprocessor_all_robust(X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Apply RobustScaler feature-wise to ALL columns. Fit happens inside the pipeline.\n",
    "    \"\"\"\n",
    "    cols = list(X_train.columns)\n",
    "    return ColumnTransformer([('rob_all', RobustScaler(), cols)], remainder='drop')\n",
    "\n",
    "def clean_names(names):\n",
    "    \"\"\"\n",
    "    Remove transformer prefixes like 'rob_all__Age' -> 'Age' for readability.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for n in names:\n",
    "        try:\n",
    "            out.append(n.split(\"__\", 1)[1] if \"__\" in n else n)\n",
    "        except Exception:\n",
    "            out.append(str(n))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) Optional EDA (correlations & class-wise boxplots)\n",
    "# ===============================================================\n",
    "if DO_EDA_PLOTS:\n",
    "    try:\n",
    "        num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        corr = X[num_cols].corr()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "        plt.title(\"Correlation Heatmap (numeric features, before preprocessing)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"eda_correlation_heatmap.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        feats = list(num_cols)\n",
    "        if len(feats) > 0:\n",
    "            n = len(feats)\n",
    "            n_cols = 3\n",
    "            n_rows = int(np.ceil(n / n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 3.5*n_rows))\n",
    "            axes = axes.ravel() if n_rows * n_cols > 1 else [axes]\n",
    "\n",
    "            last_i = 0\n",
    "            for i, col in enumerate(feats):\n",
    "                ax = axes[i]\n",
    "                d0 = data.loc[data[\"CLASS\"] == neg_label, col].dropna()\n",
    "                d1 = data.loc[data[\"CLASS\"] == pos_label, col].dropna()\n",
    "                ax.boxplot([d0, d1], labels=[f\"Class {neg_label}\", f\"Class {pos_label}\"],\n",
    "                           showfliers=True, patch_artist=True,\n",
    "                           boxprops=dict(facecolor='#D0E4F5'))\n",
    "                ax.set_title(col, fontsize=10)\n",
    "                ax.grid(axis='y', linestyle=':', linewidth=0.4)\n",
    "                last_i = i\n",
    "            for j in range(last_i+1, len(axes)):\n",
    "                axes[j].axis(\"off\")\n",
    "            fig.suptitle(\"Boxplots by CLASS (numeric features)\", y=1.02, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUT_DIR / \"eda_boxplots_by_class.png\", dpi=200, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"EDA plotting skipped due to error: {e}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4) Utility Functions (metrics, CI, stats, plotting helpers)\n",
    "# ===============================================================\n",
    "def safe_div(a, b):\n",
    "    return np.nan if b == 0 else a / b\n",
    "\n",
    "def compute_row_metrics(tn, fp, fn, tp):\n",
    "    \"\"\"\n",
    "    From raw confusion entries compute sensitivity, specificity, precision, recall,\n",
    "    F1, MCC, G-mean, accuracy.\n",
    "    \"\"\"\n",
    "    tn = float(tn); fp = float(fp); fn = float(fn); tp = float(tp)\n",
    "    sens = safe_div(tp, tp + fn)\n",
    "    spec = safe_div(tn, tn + fp)\n",
    "    prec = safe_div(tp, tp + fp)\n",
    "    rec  = sens\n",
    "    f1 = np.nan if (np.isnan(prec) or np.isnan(rec) or (prec + rec) == 0) else (2 * prec * rec / (prec + rec))\n",
    "    denom_mcc = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    mcc = np.nan if denom_mcc <= 0 else ((tp * tn) - (fp * fn)) / np.sqrt(denom_mcc)\n",
    "    gmean = np.nan if (np.isnan(sens) or np.isnan(spec)) else np.sqrt(sens * spec)\n",
    "    total = tn + fp + fn + tp\n",
    "    acc = safe_div(tn + tp, total)\n",
    "    return sens, spec, prec, rec, f1, mcc, gmean, acc\n",
    "\n",
    "def mean_std_ci_t(x, alpha=0.05):\n",
    "    \"\"\"\n",
    "    t-based CI for 1D array (NaNs removed).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = x.size\n",
    "    m = float(np.mean(x)) if n > 0 else np.nan\n",
    "    s = float(np.std(x, ddof=1)) if n > 1 else np.nan\n",
    "    if n > 1:\n",
    "        tcrit = st.t.ppf(1 - alpha/2, df=n-1)\n",
    "        half = tcrit * s / np.sqrt(n)\n",
    "        lo, hi = m - half, m + half\n",
    "    else:\n",
    "        lo, hi = np.nan, np.nan\n",
    "    return m, s, lo, hi, int(n)\n",
    "\n",
    "def paired_ttest_ci(a, b, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Paired t-test with CI: returns n, mean_diff (A-B), CI, t, p\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    n_all = min(a.size, b.size)\n",
    "    a = a[:n_all]; b = b[:n_all]\n",
    "    mask = (~np.isnan(a)) & (~np.isnan(b))\n",
    "    a = a[mask]; b = b[mask]\n",
    "    n = a.size\n",
    "    if n <= 1:\n",
    "        return {'n': int(n), 'mean_diff': np.nan, 'ci95': (np.nan, np.nan), 't': np.nan, 'p': np.nan}\n",
    "    d = a - b\n",
    "    md = float(np.mean(d))\n",
    "    sd = float(np.std(d, ddof=1))\n",
    "    tstat, pval = st.ttest_1samp(d, popmean=0.0)\n",
    "    tcrit = st.t.ppf(1 - alpha/2, df=n - 1)\n",
    "    half = tcrit * sd / np.sqrt(n)\n",
    "    ci = (md - half, md + half)\n",
    "    return {'n': int(n), 'mean_diff': md, 'ci95': ci, 't': float(tstat), 'p': float(pval)}\n",
    "\n",
    "def friedman_and_pairwise(df_long, metric, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Friedman + pairwise Wilcoxon. Returns friedman summary and pairwise table with p_raw, mean_diff.\n",
    "    \"\"\"\n",
    "    pivot = df_long.pivot_table(index='run', columns='model', values=metric)\n",
    "    pivot = pivot.dropna(axis=0, how='any')\n",
    "    models = list(pivot.columns)\n",
    "    if len(models) < 2 or pivot.shape[0] < 2:\n",
    "        empty = {'friedman': {'stat': np.nan, 'p': np.nan, 'k': len(models), 'n': pivot.shape[0]}}\n",
    "        return empty, pd.DataFrame(columns=['metric','model_a','model_b','p_raw','mean_diff','p_holm','better_model'])\n",
    "\n",
    "    stat, p = friedmanchisquare(*[pivot[m].values for m in models])\n",
    "    fried = {'stat': stat, 'p': p, 'k': len(models), 'n': pivot.shape[0]}\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            a, b = models[i], models[j]\n",
    "            vals_a, vals_b = pivot[a].values, pivot[b].values\n",
    "            try:\n",
    "                _, p_raw = wilcoxon(vals_a, vals_b, zero_method='wilcox',\n",
    "                                    correction=False, alternative='two-sided', mode='auto')\n",
    "                diff_mean = float(np.mean(vals_a - vals_b))\n",
    "            except ValueError:\n",
    "                p_raw, diff_mean = np.nan, np.nan\n",
    "            rows.append([metric, a, b, p_raw, diff_mean])\n",
    "    pw = pd.DataFrame(rows, columns=['metric','model_a','model_b','p_raw','mean_diff'])\n",
    "\n",
    "    mask = ~pw['p_raw'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(pw.loc[mask, 'p_raw'].values, method='holm')\n",
    "        pw.loc[mask, 'p_holm'] = p_holm\n",
    "    else:\n",
    "        pw['p_holm'] = np.nan\n",
    "\n",
    "    pw['better_model'] = np.where(\n",
    "        pw['mean_diff'] > 0, pw['model_a'],\n",
    "        np.where(pw['mean_diff'] < 0, pw['model_b'], 'equal')\n",
    "    )\n",
    "    return {'friedman': fried}, pw\n",
    "\n",
    "def normalize_cm(cm, mode=None):\n",
    "    cm = np.asarray(cm, dtype=float)\n",
    "    if mode is None:\n",
    "        return cm\n",
    "    if mode == \"row\":\n",
    "        row_sum = cm.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum == 0] = 1.0\n",
    "        return cm / row_sum\n",
    "    if mode == \"all\":\n",
    "        s = cm.sum()\n",
    "        return cm / s if s > 0 else cm\n",
    "    raise ValueError(\"normalize mode must be None, 'row' or 'all'.\")\n",
    "\n",
    "def format_cell(mean, std, normalized):\n",
    "    return f\"{mean:.3f}±{std:.3f}\" if normalized else f\"{mean:.1f}±{std:.1f}\"\n",
    "\n",
    "def aggregate_cm_mean_std(df_conf_all, normalize=None):\n",
    "    out = {}\n",
    "    for model, g in df_conf_all.groupby(\"model\"):\n",
    "        mats = []\n",
    "        for _, r in g.iterrows():\n",
    "            cm = np.array([[r['tn'], r['fp']], [r['fn'], r['tp']]], dtype=float)\n",
    "            mats.append(normalize_cm(cm, normalize))\n",
    "        if not mats:\n",
    "            continue\n",
    "        arr = np.stack(mats, axis=0)\n",
    "        out[model] = {\n",
    "            'mean': arr.mean(axis=0),\n",
    "            'std':  arr.std(axis=0, ddof=1) if arr.shape[0] > 1 else np.zeros((2,2)),\n",
    "            'n':    int(arr.shape[0])\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def plot_confusion_mean_std(agg, normalize=None, classes=None, out_path=None, cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Annotated confusion matrices (mean±std) for all models.\n",
    "    \"\"\"\n",
    "    models = list(agg.keys())\n",
    "    if not models:\n",
    "        raise ValueError(\"No models to plot.\")\n",
    "    n = len(models)\n",
    "    n_cols = min(3, n)\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "    figsize = (5*n_cols, 4.5*n_rows)\n",
    "    normalized = (normalize is not None)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif n_cols == 1:\n",
    "        axes = np.array([[ax] for ax in axes])\n",
    "\n",
    "    vmin, vmax = (0.0, 1.0) if normalized else (None, None)\n",
    "\n",
    "    idx = 0\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            ax = axes[r, c]\n",
    "            if idx >= n:\n",
    "                ax.axis(\"off\"); continue\n",
    "            model = models[idx]\n",
    "            M, S, n_runs = agg[model]['mean'], agg[model]['std'], agg[model]['n']\n",
    "            ax.imshow(M, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    ax.text(j, i, format_cell(M[i, j], S[i, j], normalized),\n",
    "                            color=\"black\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "\n",
    "            ticklabels = [str(classes[0]), str(classes[1])] if classes is not None else [\"Neg\",\"Pos\"]\n",
    "            ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
    "            ax.set_xticklabels([f\"Pred {ticklabels[0]}\", f\"Pred {ticklabels[1]}\"])\n",
    "            ax.set_yticklabels([f\"True {ticklabels[0]}\", f\"True {ticklabels[1]}\"])\n",
    "            ax.set_title(f\"{model} — mean±std  (n={n_runs})\")\n",
    "            ax.set_xticks(np.arange(-.5, 2, 1), minor=True)\n",
    "            ax.set_yticks(np.arange(-.5, 2, 1), minor=True)\n",
    "            ax.grid(which='minor', color='w', linestyle='-', linewidth=1)\n",
    "            ax.tick_params(which='minor', bottom=False, left=False)\n",
    "            idx += 1\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    mappable = axes[0,0].images[0]\n",
    "    fig.colorbar(mappable, cax=cbar_ax)\n",
    "    supt = \"Confusion Matrices — mean ± std\"\n",
    "    if normalize == \"row\": supt += \" (row-normalized)\"\n",
    "    elif normalize == \"all\": supt += \" (global-normalized)\"\n",
    "    fig.suptitle(supt, y=0.98, fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0, 0.90, 0.97])\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5) Optuna Objective & Param Sanitization\n",
    "# ===============================================================\n",
    "def objective(trial, model_name, X_train, y_train, X_val, y_val, preprocessor_fixed, seed=None):\n",
    "    \"\"\"\n",
    "    Build model, add SelectKBest, fit on TRAIN and return selected validation objective\n",
    "    (F1, AP, or AUC). Also logs other validation metrics into trial.user_attrs.\n",
    "    \"\"\"\n",
    "    rng_seed = (RANDOM_BASE if seed is None else int(seed))\n",
    "\n",
    "    # ---------------- Classifier space ----------------\n",
    "    if model_name == \"Random Forest\":\n",
    "        params = {\n",
    "            'n_estimators':       trial.suggest_int('n_estimators', 200, 1000, step=100),\n",
    "            'max_depth':          trial.suggest_int('max_depth', 2, 30),\n",
    "            'min_samples_split':  trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf':   trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features':       trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'criterion':          trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "            'bootstrap':          trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'class_weight':       'balanced',\n",
    "            'n_jobs':            -1,\n",
    "            'random_state':       rng_seed\n",
    "        }\n",
    "        clf = RandomForestClassifier(**params)\n",
    "\n",
    "    elif model_name == \"SVM\":\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "        params = {\n",
    "            'C':            trial.suggest_float('C', 1e-3, 1e3, log=True),\n",
    "            'kernel':       kernel,\n",
    "            'class_weight': 'balanced',\n",
    "            'probability':  True,\n",
    "            'random_state': rng_seed,\n",
    "        }\n",
    "        if kernel in ('rbf', 'poly'):\n",
    "            gm = trial.suggest_categorical('gamma_mode', ['scale', 'auto', 'float'])\n",
    "            params['gamma'] = trial.suggest_float('gamma', 1e-4, 1.0, log=True) if gm == 'float' else gm\n",
    "        if kernel == 'poly':\n",
    "            params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "        clf = SVC(**params)\n",
    "\n",
    "    elif model_name == \"MLP\":\n",
    "        solver = trial.suggest_categorical('solver', ['adam', 'lbfgs'])\n",
    "        params = {\n",
    "            'hidden_layer_sizes': trial.suggest_categorical(\n",
    "                'hidden_layer_sizes', [(20,10,5), (15,15,5), (20,15,5), (20,10)]\n",
    "            ),\n",
    "            'activation':   trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic']),\n",
    "            'alpha':        trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
    "            'solver':       solver,\n",
    "            'max_iter':     trial.suggest_int('max_iter', 400, 1200, step=100),\n",
    "            'random_state': rng_seed,\n",
    "        }\n",
    "        if solver == 'adam':\n",
    "            params.update({\n",
    "                'learning_rate':      trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
    "                'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-5, 1e-3, log=True),\n",
    "                'beta_1':             trial.suggest_float('beta_1', 0.7, 0.99),\n",
    "                'beta_2':             trial.suggest_float('beta_2', 0.9, 0.999),\n",
    "                'early_stopping':     trial.suggest_categorical('early_stopping', [True, False]),\n",
    "                'batch_size':         trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            })\n",
    "        clf = MLPClassifier(**params)\n",
    "\n",
    "    elif model_name == \"XGBoost\":\n",
    "        pos_n = int(np.sum(y_train == pos_label))\n",
    "        neg_n = int(np.sum(y_train != pos_label))\n",
    "        spw_base = (neg_n / max(1, pos_n)) if pos_n > 0 else 1.0\n",
    "        params = {\n",
    "            'n_estimators':      trial.suggest_int('n_estimators', 200, 1000, step=100),\n",
    "            'max_depth':         trial.suggest_int('max_depth', 2, 10),\n",
    "            'learning_rate':     trial.suggest_float('learning_rate', 1e-3, 3e-1, log=True),\n",
    "            'subsample':         trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree':  trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight':  trial.suggest_float('min_child_weight', 1e-1, 10.0, log=True),\n",
    "            'gamma':             trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'reg_alpha':         trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda':        trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "            'scale_pos_weight':  trial.suggest_float('scale_pos_weight', spw_base*0.5, spw_base*2.0, log=True),\n",
    "            'n_jobs':           -1,\n",
    "            'random_state':      rng_seed,\n",
    "            'eval_metric':       'logloss',\n",
    "            'tree_method':       'hist',\n",
    "        }\n",
    "        clf = XGBClassifier(**params)\n",
    "\n",
    "    elif model_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "            \"weights\":     trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\":           trial.suggest_int(\"p\", 1, 2),\n",
    "            \"leaf_size\":   trial.suggest_int(\"leaf_size\", 10, 60),\n",
    "            \"metric\":      trial.suggest_categorical(\"metric\", [\"minkowski\", \"euclidean\", \"manhattan\"]),\n",
    "        }\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "\n",
    "    elif model_name == \"Bagging\":\n",
    "        base_choice = trial.suggest_categorical(\"base_estimator\", [\"tree\", \"knn\"])\n",
    "        if base_choice == \"tree\":\n",
    "            base_est = DecisionTreeClassifier(\n",
    "                max_depth=trial.suggest_int(\"max_depth\", 1, 15),\n",
    "                min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "                class_weight='balanced',\n",
    "                random_state=rng_seed\n",
    "            )\n",
    "        else:\n",
    "            base_est = KNeighborsClassifier(\n",
    "                n_neighbors=trial.suggest_int(\"n_neighbors\", 3, 15),\n",
    "                weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "            )\n",
    "        params = {\n",
    "            \"n_estimators\":       trial.suggest_int(\"n_estimators\", 10, 200, step=10),\n",
    "            \"max_samples\":        trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\":       trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\":          trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [False, True]),\n",
    "            \"n_jobs\":            -1,\n",
    "            \"random_state\":       rng_seed,\n",
    "        }\n",
    "        clf = BaggingClassifier(estimator=base_est, **params)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "    # ---------------- Feature selector (k) ----------------\n",
    "    n_out_after_prep = X_train.shape[1]  # RobustScaler keeps dimensionality\n",
    "    k_min_eff = int(max(1, min(K_MIN, n_out_after_prep)))\n",
    "    k_max_eff = int(min(K_MAX, n_out_after_prep))\n",
    "    if k_max_eff < k_min_eff:\n",
    "        k_max_eff = k_min_eff\n",
    "    k_best = trial.suggest_int('k_best', k_min_eff, k_max_eff)\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "\n",
    "    # ---------------- Full pipeline ----------------\n",
    "    pipe = Pipeline([('prep', preprocessor_fixed),\n",
    "                     ('feat', selector),\n",
    "                     ('clf', clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation predictions\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    f1_val = f1_score(y_val, y_pred, average=\"binary\", pos_label=pos_label)\n",
    "\n",
    "    # Need probabilities for AP/AUC\n",
    "    proba_val = pipe.predict_proba(X_val)[:, int(np.where(pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])]\n",
    "    ap_val  = average_precision_score(y_val, proba_val)\n",
    "    auc_val = roc_auc_score(y_val, proba_val)\n",
    "\n",
    "    # Log all val metrics\n",
    "    trial.set_user_attr(\"val_f1\",  float(f1_val))\n",
    "    trial.set_user_attr(\"val_ap\",  float(ap_val))\n",
    "    trial.set_user_attr(\"val_auc\", float(auc_val))\n",
    "\n",
    "    # Return the chosen objective\n",
    "    if OPTIMIZE_FOR.lower() == \"ap\":\n",
    "        return ap_val\n",
    "    elif OPTIMIZE_FOR.lower() == \"auc\":\n",
    "        return auc_val\n",
    "    else:\n",
    "        return f1_val\n",
    "\n",
    "\n",
    "def sanitize_params_for_final(model_name, best_params_all):\n",
    "    \"\"\"\n",
    "    Remove pipeline-only params (k_best/gamma_mode/etc.) and return\n",
    "    (clean_params, base_choice_for_bagging).\n",
    "    \"\"\"\n",
    "    p = best_params_all.copy()\n",
    "\n",
    "    # Universal removals\n",
    "    for k in [\"k_best\", \"k_features\"]:\n",
    "        p.pop(k, None)\n",
    "\n",
    "    if model_name == \"SVM\":\n",
    "        gm = p.pop(\"gamma_mode\", None)\n",
    "        if gm in (\"scale\", \"auto\"):\n",
    "            p[\"gamma\"] = gm\n",
    "\n",
    "    if model_name == \"Bagging\":\n",
    "        base_choice = p.pop(\"base_estimator\", None)\n",
    "        allowed = {\"n_estimators\", \"max_samples\", \"max_features\",\n",
    "                   \"bootstrap\", \"bootstrap_features\", \"n_jobs\", \"random_state\"}\n",
    "        p = {k: v for k, v in p.items() if k in allowed}\n",
    "        return p, base_choice\n",
    "\n",
    "    return p, None\n",
    "\n",
    "\n",
    "def get_selected_feature_info(fitted_pipe: Pipeline, X_ref: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return selected-mask and feature names after 'prep' (and after selection),\n",
    "    plus cleaned names (without transformer prefixes).\n",
    "    \"\"\"\n",
    "    pre = fitted_pipe.named_steps.get('prep', None)\n",
    "    feat = fitted_pipe.named_steps.get('feat', None)\n",
    "    try:\n",
    "        base_names = list(pre.get_feature_names_out(input_features=X_ref.columns))\n",
    "    except Exception:\n",
    "        try:\n",
    "            base_names = list(pre.get_feature_names_out())\n",
    "        except Exception:\n",
    "            base_names = list(X_ref.columns)\n",
    "\n",
    "    mask = None\n",
    "    if feat is not None and hasattr(feat, \"get_support\"):\n",
    "        mask = feat.get_support()\n",
    "        sel_names = list(np.array(base_names)[mask])\n",
    "    else:\n",
    "        sel_names = base_names\n",
    "\n",
    "    sel_names_clean = clean_names(sel_names)\n",
    "    return mask, sel_names, sel_names_clean, base_names\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6) Main Loop: Splits + Optuna + Test Evaluation\n",
    "# ===============================================================\n",
    "model_names = [\"Random Forest\", \"SVM\", \"MLP\", \"XGBoost\", \"KNN\", \"Bagging\"]\n",
    "\n",
    "best_hyperparameters    = {m: [] for m in model_names}\n",
    "selected_features_store = {m: {} for m in model_names}\n",
    "\n",
    "test_confusions = {m: [] for m in model_names}\n",
    "test_roc_curves = {m: [] for m in model_names}  # list of (fpr, tpr, auc)\n",
    "test_pr_curves  = {m: [] for m in model_names}  # list of (recall, precision, ap)\n",
    "\n",
    "test_auc_list   = {m: [] for m in model_names}\n",
    "test_ap_list    = {m: [] for m in model_names}\n",
    "test_f1_list    = {m: [] for m in model_names}\n",
    "\n",
    "final_pipes     = {m: {} for m in model_names}\n",
    "saved_model_paths = {m: [] for m in model_names}\n",
    "split_store     = {}\n",
    "ap_records      = []\n",
    "\n",
    "for rep in range(N_REPS):\n",
    "    seed_outer = int(RANDOM_BASE + 10007*rep + 13)\n",
    "    seed_inner = int(RANDOM_BASE + 20011*rep + 97)\n",
    "    seed_model = int(RANDOM_BASE + 30029*rep + 211)\n",
    "    seed_tpe   = int(RANDOM_BASE + 40039*rep + 509)\n",
    "\n",
    "    # Outer split (20% test)\n",
    "    outer = StratifiedShuffleSplit(n_splits=1, test_size=TEST_FRACTION, random_state=seed_outer)\n",
    "    (trainval_idx, test_idx), = outer.split(X, y)\n",
    "    X_trainval, y_trainval = X.iloc[trainval_idx], y[trainval_idx]\n",
    "    X_test,     y_test     = X.iloc[test_idx],     y[test_idx]\n",
    "    split_store[rep] = {\"trainval_idx\": trainval_idx.tolist(), \"test_idx\": test_idx.tolist()}\n",
    "\n",
    "    # Inner split (train/val)\n",
    "    inner = StratifiedShuffleSplit(n_splits=1, train_size=train_in_trainval,\n",
    "                                   test_size=val_in_trainval, random_state=seed_inner)\n",
    "    (tr_idx, val_idx), = inner.split(X_trainval, y_trainval)\n",
    "    X_train, y_train = X_trainval.iloc[tr_idx], y_trainval[tr_idx]\n",
    "    X_val,   y_val   = X_trainval.iloc[val_idx], y_trainval[val_idx]\n",
    "\n",
    "    PREPROC = build_preprocessor_all_robust(X_train)\n",
    "\n",
    "    for model_name in model_names:\n",
    "        # ---- Optuna study ----\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=seed_tpe),\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "        )\n",
    "        study.optimize(\n",
    "            lambda tr: objective(tr, model_name, X_train, y_train, X_val, y_val,\n",
    "                                 preprocessor_fixed=PREPROC, seed=seed_model),\n",
    "            n_trials=N_TRIALS_BY_MODEL.get(model_name, 150),\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "        best_params_all = study.best_params.copy()\n",
    "        # pull all val metrics from best trial\n",
    "        bt = study.best_trial\n",
    "        best_val_f1  = float(bt.user_attrs.get(\"val_f1\", np.nan))\n",
    "        best_val_ap  = float(bt.user_attrs.get(\"val_ap\", np.nan))\n",
    "        best_val_auc = float(bt.user_attrs.get(\"val_auc\", np.nan))\n",
    "\n",
    "        best_hyperparameters[model_name].append({\n",
    "            \"rep\": rep,\n",
    "            \"best_params\": best_params_all.copy(),\n",
    "            \"val_f1\":  best_val_f1,\n",
    "            \"val_ap\":  best_val_ap,\n",
    "            \"val_auc\": best_val_auc,\n",
    "            \"optimize_for\": OPTIMIZE_FOR\n",
    "        })\n",
    "\n",
    "        clean_params, base_choice = sanitize_params_for_final(model_name, best_params_all)\n",
    "        k_best = int(best_params_all.get(\"k_best\", min(K_MAX, X_train.shape[1])))\n",
    "\n",
    "        # ---- Final model on TRAIN+VAL ----\n",
    "        if model_name == \"Random Forest\":\n",
    "            clf_best = RandomForestClassifier(\n",
    "                **clean_params, n_jobs=-1, class_weight='balanced', random_state=seed_model\n",
    "            )\n",
    "        elif model_name == \"SVM\":\n",
    "            clf_best = SVC(\n",
    "                probability=True, **clean_params, class_weight='balanced', random_state=seed_model\n",
    "            )\n",
    "        elif model_name == \"MLP\":\n",
    "            clf_best = MLPClassifier(**clean_params, random_state=seed_model)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            clf_best = XGBClassifier(eval_metric=\"logloss\", n_jobs=-1, random_state=seed_model, **clean_params)\n",
    "        elif model_name == \"KNN\":\n",
    "            clf_best = KNeighborsClassifier(**clean_params)\n",
    "        elif model_name == \"Bagging\":\n",
    "            if base_choice is None:\n",
    "                base_choice = best_params_all.get(\"base_estimator\", \"tree\")\n",
    "            if base_choice == \"tree\":\n",
    "                base_est = DecisionTreeClassifier(\n",
    "                    max_depth=best_params_all.get(\"max_depth\", None),\n",
    "                    min_samples_split=best_params_all.get(\"min_samples_split\", 2),\n",
    "                    min_samples_leaf=best_params_all.get(\"min_samples_leaf\", 1),\n",
    "                    class_weight='balanced',\n",
    "                    random_state=seed_model,\n",
    "                )\n",
    "            else:\n",
    "                base_est = KNeighborsClassifier(\n",
    "                    n_neighbors=best_params_all.get(\"n_neighbors\", 5),\n",
    "                    weights=best_params_all.get(\"weights\", \"uniform\")\n",
    "                )\n",
    "            bag_params = {k: v for k, v in best_params_all.items() if k in {\n",
    "                \"n_estimators\", \"max_samples\", \"max_features\",\n",
    "                \"bootstrap\", \"bootstrap_features\", \"n_jobs\", \"random_state\"\n",
    "            }}\n",
    "            clf_best = BaggingClassifier(estimator=base_est, random_state=seed_model, **bag_params)\n",
    "\n",
    "            best_hyperparameters[model_name][-1][\"best_params_structured\"] = {\n",
    "                \"bagging\": bag_params,\n",
    "                \"base_estimator\": base_choice\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "        selector_final = SelectKBest(score_func=f_classif, k=k_best)\n",
    "        final_pipe = Pipeline([(\"prep\", PREPROC), (\"feat\", selector_final), (\"clf\", clf_best)])\n",
    "        final_pipe.fit(X_trainval, y_trainval)\n",
    "\n",
    "        # ---- Save selected features ----\n",
    "        mask, sel_names, sel_names_clean, base_names = get_selected_feature_info(final_pipe, X)\n",
    "        selected_features_store[model_name][rep] = {\n",
    "            \"k_best\": int(k_best),\n",
    "            \"mask_after_prep\": None if mask is None else mask.astype(bool).tolist(),\n",
    "            \"selected_feature_names_after_prep\": sel_names,\n",
    "            \"selected_feature_names_clean\": sel_names_clean,\n",
    "            \"base_feature_names_after_prep\": base_names,\n",
    "            \"f_scores_all\": getattr(selector_final, \"scores_\", None).tolist() if getattr(selector_final, \"scores_\", None) is not None else None,\n",
    "            \"p_values_all\": getattr(selector_final, \"pvalues_\", None).tolist() if getattr(selector_final, \"pvalues_\", None) is not None else None\n",
    "        }\n",
    "        best_hyperparameters[model_name][-1][\"k_best\"] = int(k_best)\n",
    "        best_hyperparameters[model_name][-1][\"selected_feature_names_clean\"] = sel_names_clean\n",
    "\n",
    "        # ---- Test evaluation ----\n",
    "        pos_idx    = int(np.where(final_pipe.named_steps[\"clf\"].classes_ == pos_label)[0][0])\n",
    "        proba_test = final_pipe.predict_proba(X_test)[:, pos_idx]\n",
    "        auc_test   = roc_auc_score(y_test, proba_test)\n",
    "        ap_test    = average_precision_score(y_test, proba_test)\n",
    "        y_pred     = final_pipe.predict(X_test)\n",
    "        f1_test    = f1_score(y_test, y_pred, average=\"binary\", pos_label=pos_label)\n",
    "\n",
    "        cm   = confusion_matrix(y_test, y_pred, labels=classes_sorted)\n",
    "        fpr, tpr, _ = roc_curve(y_test, proba_test, pos_label=pos_label)\n",
    "        prec, rec, _ = precision_recall_curve(y_test, proba_test, pos_label=pos_label)\n",
    "\n",
    "        test_confusions[model_name].append(cm)\n",
    "        test_roc_curves[model_name].append((fpr, tpr, auc_test))\n",
    "        test_pr_curves[model_name].append((rec, prec, ap_test))\n",
    "        test_auc_list[model_name].append(float(auc_test))\n",
    "        test_ap_list[model_name].append(float(ap_test))\n",
    "        test_f1_list[model_name].append(float(f1_test))\n",
    "\n",
    "        final_pipes[model_name][rep] = final_pipe\n",
    "        ap_records.append({\n",
    "            \"model\": model_name, \"rep\": rep,\n",
    "            \"auc_test\": float(auc_test), \"ap_test\":  float(ap_test), \"f1_test\":  float(f1_test),\n",
    "            \"val_f1\":   float(best_val_f1), \"val_ap\": float(best_val_ap), \"val_auc\": float(best_val_auc),\n",
    "            \"optimize_for\": OPTIMIZE_FOR\n",
    "        })\n",
    "\n",
    "        if SAVE_MODELS:\n",
    "            safe_model = model_name.replace(\" \", \"\")\n",
    "            fname = MODEL_DIR / f\"{safe_model}__rep{rep:02d}__OBJ{OPTIMIZE_FOR}__F1{f1_test:.4f}__AUC{auc_test:.4f}__AP{ap_test:.4f}.joblib\"\n",
    "            dump(final_pipe, fname, compress=3)\n",
    "            saved_model_paths[model_name].append(str(fname))\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7) Run-level Tables, Summaries, ROC/PR(mean±std)\n",
    "# ===============================================================\n",
    "df_scores = pd.DataFrame(ap_records).sort_values([\"model\", \"f1_test\"]).reset_index(drop=True)\n",
    "df_scores.to_csv(OUT_DIR / \"scores_f1_runs.csv\", index=False)\n",
    "\n",
    "df_scores_ap = pd.DataFrame(ap_records).sort_values([\"model\", \"ap_test\"]).reset_index(drop=True)\n",
    "df_scores_ap.to_csv(OUT_DIR / \"scores_pr_auc_runs.csv\", index=False)\n",
    "\n",
    "# Summary table (mean±std) for AUC/AP/F1\n",
    "rows_sum = []\n",
    "for m in model_names:\n",
    "    rows_sum.append({\n",
    "        \"model\":   m,\n",
    "        \"AUC_mean\": np.mean(test_auc_list[m]) if test_auc_list[m] else np.nan,\n",
    "        \"AUC_std\":  np.std(test_auc_list[m], ddof=1) if len(test_auc_list[m]) > 1 else 0.0,\n",
    "        \"AP_mean\":  np.mean(test_ap_list[m]) if test_ap_list[m] else np.nan,\n",
    "        \"AP_std\":   np.std(test_ap_list[m], ddof=1) if len(test_ap_list[m]) > 1 else 0.0,\n",
    "        \"F1_mean\":  np.mean(test_f1_list[m]) if test_f1_list[m] else np.nan,\n",
    "        \"F1_std\":   np.std(test_f1_list[m], ddof=1) if len(test_f1_list[m]) > 1 else 0.0,\n",
    "        \"N_reps\":   len(test_auc_list[m])\n",
    "    })\n",
    "df_summary = pd.DataFrame(rows_sum)\n",
    "df_summary.to_csv(OUT_DIR / \"summary_mean_std.csv\", index=False)\n",
    "\n",
    "# Save best hyperparameters and selection info\n",
    "with open(OUT_DIR / \"best_hyperparameters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_hyperparameters, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"selected_features_store.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(selected_features_store, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "dump({\n",
    "    \"test_confusions\": test_confusions,\n",
    "    \"test_roc_curves\": test_roc_curves,\n",
    "    \"test_pr_curves\":  test_pr_curves,\n",
    "    \"test_auc_list\":   test_auc_list,\n",
    "    \"test_ap_list\":    test_ap_list,\n",
    "    \"test_f1_list\":    test_f1_list,\n",
    "    \"split_store\":     split_store,\n",
    "    \"saved_model_paths\": saved_model_paths\n",
    "}, OUT_DIR / \"diagnostics.joblib\", compress=3)\n",
    "\n",
    "# ROC(mean±std) on a fixed FPR grid\n",
    "fpr_grid = np.linspace(0, 1, 201)\n",
    "plt.figure(figsize=(10, 8))\n",
    "for m, curves in test_roc_curves.items():\n",
    "    if not curves: continue\n",
    "    interp_tprs = []\n",
    "    for (fpr, tpr, _) in curves:\n",
    "        tpr_i = np.interp(fpr_grid, fpr, tpr)\n",
    "        tpr_i[0], tpr_i[-1] = 0.0, 1.0\n",
    "        interp_tprs.append(tpr_i)\n",
    "    interp_tprs = np.vstack(interp_tprs)\n",
    "    mean_tpr = interp_tprs.mean(axis=0)\n",
    "    std_tpr  = interp_tprs.std(axis=0, ddof=1)\n",
    "    mean_auc = float(np.mean(test_auc_list[m])); std_auc = float(np.std(test_auc_list[m], ddof=1))\n",
    "    plt.plot(fpr_grid, mean_tpr, label=f\"{m} (AUC={mean_auc:.3f}±{std_auc:.3f})\")\n",
    "    plt.fill_between(fpr_grid, np.maximum(mean_tpr - std_tpr, 0), np.minimum(mean_tpr + std_tpr, 1), alpha=0.15)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title(f'UNSEEN TEST — Mean ROC ± 1 std (20 repeats) — {RUN_TAG}')\n",
    "plt.legend(loc='lower right'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"roc_mean_std.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# PR(mean±std) on a fixed recall grid\n",
    "recall_grid = np.linspace(0, 1, 201)\n",
    "plt.figure(figsize=(10, 8))\n",
    "for m, curves in test_pr_curves.items():\n",
    "    if not curves: continue\n",
    "    interp_precs = []\n",
    "    for (rec, prec, _) in curves:\n",
    "        # ensure increasing recall for interpolation\n",
    "        order = np.argsort(rec)\n",
    "        rec_sorted = rec[order]; prec_sorted = prec[order]\n",
    "        prec_i = np.interp(recall_grid, rec_sorted, prec_sorted, left=prec_sorted[0], right=prec_sorted[-1])\n",
    "        interp_precs.append(prec_i)\n",
    "    interp_precs = np.vstack(interp_precs)\n",
    "    mean_prec = interp_precs.mean(axis=0)\n",
    "    std_prec  = interp_precs.std(axis=0, ddof=1)\n",
    "    mean_ap   = float(np.mean(test_ap_list[m])); std_ap = float(np.std(test_ap_list[m], ddof=1))\n",
    "    plt.plot(recall_grid, mean_prec, label=f\"{m} (AP={mean_ap:.3f}±{std_ap:.3f})\")\n",
    "    plt.fill_between(recall_grid, np.maximum(mean_prec - std_prec, 0), np.minimum(mean_prec + std_prec, 1), alpha=0.15)\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "plt.title(f'UNSEEN TEST — Mean PR ± 1 std (20 repeats) — {RUN_TAG}')\n",
    "plt.legend(loc='lower left'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"pr_mean_std.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n[OK] Saved core outputs under:\", OUT_DIR.resolve())\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 8) Confusion Matrices (All runs) + Mean±Std Visualizations\n",
    "# ===============================================================\n",
    "rows_all = []\n",
    "for model_name, cm_list in test_confusions.items():\n",
    "    for rep0, cm in enumerate(cm_list):\n",
    "        cm = np.asarray(cm)\n",
    "        if cm.shape != (2, 2):\n",
    "            raise ValueError(f\"[{model_name}] rep={rep0}: Confusion matrix shape {cm.shape} is not 2x2.\")\n",
    "        tn, fp = int(cm[0, 0]), int(cm[0, 1])\n",
    "        fn, tp = int(cm[1, 0]), int(cm[1, 1])\n",
    "        rows_all.append({\"model\": model_name, \"run\": rep0 + 1, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp})\n",
    "\n",
    "df_conf_all = pd.DataFrame(rows_all, columns=[\"model\", \"run\", \"tn\", \"fp\", \"fn\", \"tp\"])\n",
    "df_conf_all.to_excel(BASE_OUT_DIR / f\"unseen_confusions_allruns_{RUN_TAG}.xlsx\", sheet_name=\"confusions_allruns\", index=False)\n",
    "df_conf_all.to_csv  (BASE_OUT_DIR / f\"unseen_confusions_allruns_{RUN_TAG}.csv\", index=False)\n",
    "\n",
    "# Mean±std confusion plots (raw, row-normalized, all-normalized)\n",
    "for norm_tag in [None, \"row\", \"all\"]:\n",
    "    agg = aggregate_cm_mean_std(df_conf_all, normalize=norm_tag)\n",
    "    out_png = BASE_OUT_DIR / f\"confusion_mean_std_{RUN_TAG}_{'raw' if norm_tag is None else norm_tag}.png\"\n",
    "    plot_confusion_mean_std(agg, normalize=norm_tag, classes=list(classes_sorted), out_path=out_png)\n",
    "    print(f\"[OK] Saved: {out_png}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 9) Per-run Metrics + Friedman/Wilcoxon + Significance (Holm-gated)\n",
    "# ===============================================================\n",
    "ALPHA_SIG = 0.05      # Holm-corrected alpha\n",
    "TIE_EPS   = 1e-12     # tie threshold for mean_diff\n",
    "\n",
    "# Per-run metrics table\n",
    "metrics_list = ['sensitivity','specificity','precision','recall','f1','mcc','gmean','accuracy']\n",
    "mets = df_conf_all.apply(lambda r: compute_row_metrics(r['tn'], r['fp'], r['fn'], r['tp']), axis=1)\n",
    "(df_conf_all['sensitivity'], df_conf_all['specificity'], df_conf_all['precision'], df_conf_all['recall'],\n",
    " df_conf_all['f1'], df_conf_all['mcc'], df_conf_all['gmean'], df_conf_all['accuracy']) = zip(*mets)\n",
    "\n",
    "df_per_run = df_conf_all[['model','run','tn','fp','fn','tp'] + metrics_list].copy()\n",
    "df_per_run.to_csv(OUT_DIR / \"metrics_per_run_20.csv\", index=False)\n",
    "\n",
    "# Summary (mean ± std)\n",
    "summary_20 = (df_per_run.groupby('model')[metrics_list].agg(['mean','std','count']).reset_index())\n",
    "summary_20.columns = ['model'] + ['_'.join(col).strip() for col in summary_20.columns[1:]]\n",
    "summary_20.to_csv(OUT_DIR / \"metrics_summary_by_model_20.csv\", index=False)\n",
    "\n",
    "# Friedman + pairwise Wilcoxon(Holm), then build gated \"winner/No/None\"\n",
    "friedman_rows = []\n",
    "pairwise_all  = []\n",
    "\n",
    "for mtr in metrics_list:\n",
    "    df_long = df_per_run[['model','run', mtr]].copy()\n",
    "    res, pw_raw = friedman_and_pairwise(df_long, mtr)\n",
    "\n",
    "    pw = pw_raw.copy()\n",
    "    # Direction raw\n",
    "    pw['direction_raw'] = np.where(\n",
    "        pw['mean_diff'] >  TIE_EPS, pw['model_a'],\n",
    "        np.where(pw['mean_diff'] < -TIE_EPS, pw['model_b'], 'tie')\n",
    "    )\n",
    "\n",
    "    # Recompute Holm to be safe\n",
    "    mask = ~pw['p_raw'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(pw.loc[mask, 'p_raw'].values, method='holm')\n",
    "        pw.loc[mask, 'p_holm'] = p_holm\n",
    "    else:\n",
    "        pw['p_holm'] = np.nan\n",
    "\n",
    "    # Winner labeling (Holm-gated)\n",
    "    # - significant & mean_diff>0  -> model_a wins\n",
    "    # - significant & mean_diff<0  -> model_b wins\n",
    "    # - not significant            -> \"No\"\n",
    "    # - no data (p_holm NaN)       -> \"None\"\n",
    "    winner_report = []\n",
    "    for _, r in pw.iterrows():\n",
    "        if pd.isna(r['p_holm']):\n",
    "            winner_report.append(\"None\")\n",
    "        elif r['p_holm'] < ALPHA_SIG:\n",
    "            if r['mean_diff'] >  TIE_EPS: winner_report.append(r['model_a'])\n",
    "            elif r['mean_diff'] < -TIE_EPS: winner_report.append(r['model_b'])\n",
    "            else: winner_report.append(\"No\")\n",
    "        else:\n",
    "            winner_report.append(\"No\")\n",
    "    pw['winner'] = winner_report\n",
    "\n",
    "    friedman_rows.append({\n",
    "        'metric': mtr, 'friedman_stat': res['friedman']['stat'],\n",
    "        'friedman_p': res['friedman']['p'], 'k_models': res['friedman']['k'],\n",
    "        'n_runs': res['friedman']['n']\n",
    "    })\n",
    "    pairwise_all.append(pw)\n",
    "\n",
    "df_friedman_20 = pd.DataFrame(friedman_rows)\n",
    "df_pairwise_20 = pd.concat(pairwise_all, ignore_index=True)\n",
    "\n",
    "# Save CSVs\n",
    "df_friedman_20.to_csv(OUT_DIR / \"stats_friedman_20.csv\", index=False)\n",
    "df_pairwise_20.to_csv(OUT_DIR / \"stats_pairwise_20_directional.csv\", index=False)\n",
    "print(\"[OK] Saved nonparametric stats (Friedman + Wilcoxon-Holm, gated winner).\")\n",
    "\n",
    "# Build model×model matrices (per metric) & save to Excel + plot PNGs\n",
    "signif_xlsx = BASE_OUT_DIR / f\"pairwise_matrices_significance_{RUN_TAG}.xlsx\"\n",
    "models_all = sorted(set(df_pairwise_20[\"model_a\"]) | set(df_pairwise_20[\"model_b\"]))\n",
    "\n",
    "def _plot_matrix_text(M_df, title, out_png, cmap=None, highlight_mask=None):\n",
    "    \"\"\"\n",
    "    Plot a text-annotated matrix (strings) with optional highlight mask.\n",
    "    highlight_mask==True cells will be colored; others light.\n",
    "    \"\"\"\n",
    "    # Map to numeric for a colormap background: highlight=1 else 0, diagonal NaN\n",
    "    code = np.zeros(M_df.shape, dtype=float)\n",
    "    txt = M_df.values.astype(str)\n",
    "    for i in range(M_df.shape[0]):\n",
    "        for j in range(M_df.shape[1]):\n",
    "            if i == j:\n",
    "                code[i, j] = np.nan\n",
    "            else:\n",
    "                if highlight_mask is not None and highlight_mask[i, j]:\n",
    "                    code[i, j] = 1.0\n",
    "                else:\n",
    "                    code[i, j] = 0.0\n",
    "    plt.figure(figsize=(1.2*M_df.shape[1], 1.0*M_df.shape[0]))\n",
    "    ax = sns.heatmap(code, annot=txt, fmt=\"\", cbar=False,\n",
    "                     cmap=cmap if cmap else sns.color_palette([\"#f0f0f0\",\"#bfe3bf\"], as_cmap=True),\n",
    "                     linewidths=.5, linecolor='white')\n",
    "    ax.set_xticklabels(M_df.columns, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(M_df.index, rotation=0)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "with pd.ExcelWriter(signif_xlsx) as writer:\n",
    "    for mtr in metrics_list:\n",
    "        sub = df_pairwise_20[df_pairwise_20[\"metric\"] == mtr].copy()\n",
    "\n",
    "        # Initialize matrices\n",
    "        P = pd.DataFrame(np.nan, index=models_all, columns=models_all)  # pHolm\n",
    "        M = pd.DataFrame(\"None\", index=models_all, columns=models_all)  # winner/No/None\n",
    "\n",
    "        # Fill\n",
    "        for _, r in sub.iterrows():\n",
    "            a, b, p, w = r[\"model_a\"], r[\"model_b\"], r[\"p_holm\"], r[\"winner\"]\n",
    "            P.loc[a, b] = P.loc[b, a] = p\n",
    "            M.loc[a, b] = M.loc[b, a] = w\n",
    "\n",
    "        # Diagonals\n",
    "        np.fill_diagonal(P.values, 0.0)\n",
    "        np.fill_diagonal(M.values, \"-\")\n",
    "\n",
    "        # Save sheets\n",
    "        P.to_excel(writer, sheet_name=f\"{mtr}_pHolm\")\n",
    "        M.to_excel(writer, sheet_name=f\"{mtr}_sig\")\n",
    "\n",
    "        # ---- Also export PNG heatmaps for GitHub ----\n",
    "        # 1) pHolm heatmap as -log10(p) (visual dynamic)\n",
    "        P_plot = P.copy()\n",
    "        with np.errstate(divide='ignore'):\n",
    "            P_plot = -np.log10(P_plot)\n",
    "        plt.figure(figsize=(1.2*len(models_all), 1.0*len(models_all)))\n",
    "        ax = sns.heatmap(P_plot, cmap=\"mako\", linewidths=.5, linecolor='white',\n",
    "                         cbar_kws={'label': '-log10(pHolm)'})\n",
    "        ax.set_xticklabels(P_plot.columns, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(P_plot.index, rotation=0)\n",
    "        plt.title(f\"{mtr} — pHolm (−log10) — {RUN_TAG}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(STATS_PLOTS / f\"{mtr}_pHolm_heatmap_{RUN_TAG}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        # 2) winner/No/None annotated matrix with significant cells highlighted\n",
    "        # highlight = (winner not in {\"No\",\"None\",\"-\"} and off-diagonal)\n",
    "        hl = np.zeros(M.shape, dtype=bool)\n",
    "        for i in range(M.shape[0]):\n",
    "            for j in range(M.shape[1]):\n",
    "                if i == j:\n",
    "                    hl[i, j] = False\n",
    "                else:\n",
    "                    val = M.iloc[i, j]\n",
    "                    hl[i, j] = (val not in {\"No\", \"None\", \"-\"})\n",
    "\n",
    "        _plot_matrix_text(\n",
    "            M, title=f\"{mtr} — Winner/No/None — {RUN_TAG}\",\n",
    "            out_png=STATS_PLOTS / f\"{mtr}_sig_winner_matrix_{RUN_TAG}.png\",\n",
    "            highlight_mask=hl\n",
    "        )\n",
    "\n",
    "print(f\"[OK] Saved significance matrices & plots: {signif_xlsx}, {STATS_PLOTS}\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 10) 95% t-CI for ALL metrics + AUC/AP + Paired t-tests for AUC\n",
    "# ===============================================================\n",
    "ALPHA_CI = 0.05\n",
    "\n",
    "# CI for per-run metrics\n",
    "rows_ci = []\n",
    "for model_name, g in df_per_run.groupby('model'):\n",
    "    for mtr in metrics_list:\n",
    "        m, s, lo, hi, n = mean_std_ci_t(g[mtr].values, alpha=ALPHA_CI)\n",
    "        rows_ci.append({'model': model_name, 'metric': mtr, 'mean': m, 'std': s,\n",
    "                        'ci95_low': lo, 'ci95_high': hi, 'n_runs_used': n})\n",
    "df_ci_long = pd.DataFrame(rows_ci).sort_values(['model','metric']).reset_index(drop=True)\n",
    "df_ci_long.to_csv(OUT_DIR / \"metrics_ci95_by_model_20_long.csv\", index=False)\n",
    "\n",
    "# Wide variant\n",
    "wide_rows = []\n",
    "for model_name, g in df_per_run.groupby('model'):\n",
    "    row = {'model': model_name}\n",
    "    for mtr in metrics_list:\n",
    "        m, s, lo, hi, n = mean_std_ci_t(g[mtr].values, alpha=ALPHA_CI)\n",
    "        row[f\"{mtr}_mean\"] = m; row[f\"{mtr}_std\"] = s\n",
    "        row[f\"{mtr}_ci95_low\"] = lo; row[f\"{mtr}_ci95_high\"] = hi; row[f\"{mtr}_n\"] = n\n",
    "    wide_rows.append(row)\n",
    "df_ci_wide = pd.DataFrame(wide_rows).sort_values('model').reset_index(drop=True)\n",
    "df_ci_wide.to_csv(OUT_DIR / \"metrics_ci95_by_model_20_wide.csv\", index=False)\n",
    "\n",
    "# CI for AUC and AP\n",
    "rows_auc_ap = []\n",
    "auc_src = {k: list(v) for k, v in test_auc_list.items()} if test_auc_list else {}\n",
    "ap_src  = {k: list(v) for k, v in test_ap_list.items()}  if test_ap_list  else {}\n",
    "for model, arr in auc_src.items():\n",
    "    m, s, lo, hi, n = mean_std_ci_t(arr, alpha=ALPHA_CI)\n",
    "    rows_auc_ap.append({'model': model, 'metric': 'auc', 'mean': m, 'std': s,\n",
    "                        'ci95_low': lo, 'ci95_high': hi, 'n_runs_used': n})\n",
    "for model, arr in ap_src.items():\n",
    "    m, s, lo, hi, n = mean_std_ci_t(arr, alpha=ALPHA_CI)\n",
    "    rows_auc_ap.append({'model': model, 'metric': 'ap', 'mean': m, 'std': s,\n",
    "                        'ci95_low': lo, 'ci95_high': hi, 'n_runs_used': n})\n",
    "if rows_auc_ap:\n",
    "    pd.DataFrame(rows_auc_ap).sort_values(['model','metric']).to_csv(\n",
    "        OUT_DIR / \"auc_ap_ci95_by_model_20.csv\", index=False\n",
    "    )\n",
    "\n",
    "# Paired t-tests for AUC\n",
    "pairs = []\n",
    "models = list(auc_src.keys())\n",
    "for a, b in itertools.combinations(models, 2):\n",
    "    res = paired_ttest_ci(auc_src[a], auc_src[b], alpha=ALPHA_CI)\n",
    "    pairs.append({'model_a': a, 'model_b': b, 'n_pairs': res['n'],\n",
    "                  'delta_mean_auc_(a-b)': res['mean_diff'],\n",
    "                  'ci95_low': res['ci95'][0], 'ci95_high': res['ci95'][1],\n",
    "                  't_stat': res['t'], 'p_value': res['p']})\n",
    "df_pairwise_auc = pd.DataFrame(pairs)\n",
    "if not df_pairwise_auc.empty:\n",
    "    mask = ~df_pairwise_auc['p_value'].isna()\n",
    "    if mask.any():\n",
    "        _, p_holm, _, _ = multipletests(df_pairwise_auc.loc[mask, 'p_value'].values, method='holm')\n",
    "        df_pairwise_auc.loc[mask, 'p_value_holm'] = p_holm\n",
    "    else:\n",
    "        df_pairwise_auc['p_value_holm'] = np.nan\n",
    "    df_pairwise_auc.to_csv(OUT_DIR / \"auc_pairwise_paired_ttest_20.csv\", index=False)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 11) XAI: SHAP & LIME (SELECTED feature space)\n",
    "# ===============================================================\n",
    "if DO_EXPLAINERS:\n",
    "    try:\n",
    "        import shap\n",
    "        from lime.lime_tabular import LimeTabularExplainer\n",
    "        from scipy import sparse as sp\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Explainability packages not available; SHAP/LIME skipped: {e}\")\n",
    "        DO_EXPLAINERS = False\n",
    "\n",
    "if DO_EXPLAINERS:\n",
    "\n",
    "    def transform_until_estimator(pipe: Pipeline, X_in):\n",
    "        Xt = X_in\n",
    "        for name, step in pipe.named_steps.items():\n",
    "            if name == 'clf':\n",
    "                break\n",
    "            Xt = step.transform(Xt)\n",
    "            if sp.issparse(Xt):\n",
    "                Xt = Xt.toarray()\n",
    "        return Xt\n",
    "\n",
    "    def get_selected_names_for_pipe(pipe: Pipeline, X_ref):\n",
    "        mask, sel_names, sel_names_clean, _ = get_selected_feature_info(pipe, X_ref)\n",
    "        return sel_names, sel_names_clean\n",
    "\n",
    "    # Select runs around median AP per model for explanation\n",
    "    def select_median_window(df_scores, model_name, window=2):\n",
    "        dfm = df_scores[df_scores['model'] == model_name].sort_values('ap_test').reset_index(drop=True)\n",
    "        if dfm.empty:\n",
    "            return dfm\n",
    "        k = len(dfm); mid = k // 2\n",
    "        lo = max(0, mid - window); hi = min(k, mid + window + 1)\n",
    "        return dfm.iloc[lo:hi][['model','rep','ap_test','auc_test']]\n",
    "\n",
    "    WINDOW = 2\n",
    "    df_scores_ap = pd.DataFrame(ap_records)\n",
    "    selected_rows = {m: select_median_window(df_scores_ap, m, window=WINDOW) for m in model_names}\n",
    "\n",
    "    sel_log = []\n",
    "    for m, d in selected_rows.items():\n",
    "        if d is None or d.empty: continue\n",
    "        for _, r in d.iterrows():\n",
    "            sel_log.append({'model': m, 'rep': int(r['rep']),\n",
    "                            'ap_test': float(r.get('ap_test', np.nan)),\n",
    "                            'auc_test': float(r.get('auc_test', np.nan))})\n",
    "    if sel_log:\n",
    "        pd.DataFrame(sel_log).to_csv(EXPL_DIR / \"selected_runs_for_explanations.csv\", index=False)\n",
    "\n",
    "    BGN = 200   # background subsample (train+val)\n",
    "    TSN = 50    # test subsample\n",
    "    TOPK_DEP = 5\n",
    "\n",
    "    for model_name in model_names:\n",
    "        sel_df = selected_rows.get(model_name, pd.DataFrame())\n",
    "        if sel_df is None or sel_df.empty:\n",
    "            print(f\"[WARN] {model_name}: no selected runs; skipping.\")\n",
    "            continue\n",
    "\n",
    "        class_names = [str(c) for c in classes_sorted]\n",
    "        safe_model = model_name.replace(\" \", \"\")\n",
    "\n",
    "        for _, row in sel_df.iterrows():\n",
    "            rep = int(row['rep'])\n",
    "            if rep not in split_store:\n",
    "                print(f\"[WARN] {model_name}: rep={rep} split not found; skipped.\")\n",
    "                continue\n",
    "\n",
    "            idx_trainval = split_store[rep]['trainval_idx']\n",
    "            idx_test     = split_store[rep]['test_idx']\n",
    "            X_bg_full = X.iloc[idx_trainval]\n",
    "            X_ts_full = X.iloc[idx_test]\n",
    "\n",
    "            rng = np.random.default_rng(42)\n",
    "            b_idx = rng.choice(len(X_bg_full), size=min(BGN, len(X_bg_full)), replace=False)\n",
    "            t_idx = rng.choice(len(X_ts_full), size=min(TSN, len(X_ts_full)), replace=False)\n",
    "            X_bg  = X_bg_full.iloc[b_idx]\n",
    "            X_ts  = X_ts_full.iloc[t_idx]\n",
    "\n",
    "            pipe = final_pipes[model_name][rep]\n",
    "            clf  = pipe.named_steps['clf']\n",
    "            pos_idx_local = int(np.where(clf.classes_ == pos_label)[0][0])\n",
    "\n",
    "            # Transform into SELECTED feature space\n",
    "            X_bg_sel = transform_until_estimator(pipe, X_bg)\n",
    "            X_ts_sel = transform_until_estimator(pipe, X_ts)\n",
    "            feat_names_sel, feat_names_sel_clean = get_selected_names_for_pipe(pipe, X)\n",
    "\n",
    "            # SHAP\n",
    "            model_type = type(clf).__name__.lower()\n",
    "            use_kernel = False\n",
    "            sv_arr = None\n",
    "\n",
    "            try:\n",
    "                if (\"randomforest\" in model_type) or (\"xgb\" in model_type) or (\"xgboost\" in model_type) or (\"decisiontree\" in model_type):\n",
    "                    explainer = shap.TreeExplainer(clf)\n",
    "                    shap_values = explainer.shap_values(X_ts_sel)\n",
    "                    sv_arr = shap_values[pos_idx_local] if isinstance(shap_values, list) else shap_values\n",
    "                elif \"logisticregression\" in model_type:\n",
    "                    explainer = shap.LinearExplainer(clf, X_bg_sel)\n",
    "                    shap_values = explainer.shap_values(X_ts_sel)\n",
    "                    sv_arr = shap_values\n",
    "                else:\n",
    "                    use_kernel = True\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[INFO] {model_name} rep={rep}: Tree/Linear SHAP not applicable ({e}); using Kernel.\")\n",
    "                use_kernel = True\n",
    "\n",
    "            if use_kernel:\n",
    "                def proba_fn_sel(z):\n",
    "                    z = np.asarray(z)\n",
    "                    return clf.predict_proba(z)[:, pos_idx_local]\n",
    "                explainer = shap.KernelExplainer(proba_fn_sel, X_bg_sel, link=\"logit\")\n",
    "                shap_values = explainer.shap_values(X_ts_sel, nsamples=2048)\n",
    "                sv_arr = shap_values[pos_idx_local] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "            # SHAP plots\n",
    "            try:\n",
    "                plt.figure(figsize=(9, 6))\n",
    "                shap.summary_plot(sv_arr, features=None, feature_names=feat_names_sel_clean,\n",
    "                                  plot_type=\"bar\", show=False, max_display=20)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(EXPL_DIR / f\"{safe_model}__rep{rep:02d}__shap_bar.png\", dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} bar plot error: {e}\")\n",
    "\n",
    "            try:\n",
    "                plt.figure(figsize=(9, 6))\n",
    "                shap.summary_plot(sv_arr, X_ts_sel, feature_names=feat_names_sel_clean, show=False, max_display=20)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(EXPL_DIR / f\"{safe_model}__rep{rep:02d}__shap_beeswarm.png\", dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} beeswarm error: {e}\")\n",
    "\n",
    "            try:\n",
    "                mean_abs = np.mean(np.abs(sv_arr), axis=0)\n",
    "                top_idx = np.argsort(mean_abs)[::-1][:min(TOPK_DEP, len(mean_abs))]\n",
    "                for j in top_idx:\n",
    "                    xj = np.ravel(X_ts_sel[:, j]); yj = np.ravel(sv_arr[:, j])\n",
    "                    m = min(xj.size, yj.size); xj = xj[:m]; yj = yj[:m]\n",
    "                    mask_f = np.isfinite(xj) & np.isfinite(yj); xj = xj[mask_f]; yj = yj[mask_f]\n",
    "                    if xj.size == 0: continue\n",
    "                    plt.figure(figsize=(7, 5))\n",
    "                    plt.scatter(xj, yj, s=12, alpha=0.6)\n",
    "                    plt.xlabel(feat_names_sel_clean[j]); plt.ylabel(\"SHAP value\")\n",
    "                    plt.title(f\"{model_name} rep={rep} — {feat_names_sel_clean[j]}\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(EXPL_DIR / f\"{safe_model}__rep{rep:02d}__dep_feat{j}.png\", dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} dependence plot error: {e}\")\n",
    "\n",
    "            # LIME\n",
    "            try:\n",
    "                lime_expl = LimeTabularExplainer(\n",
    "                    training_data=X_bg_sel,\n",
    "                    feature_names=feat_names_sel_clean,\n",
    "                    class_names=class_names,\n",
    "                    mode='classification',\n",
    "                    discretize_continuous=True\n",
    "                )\n",
    "                predict_fn_lime = lambda data: clf.predict_proba(np.asarray(data))\n",
    "                for i in range(min(3, X_ts_sel.shape[0])):\n",
    "                    exp = lime_expl.explain_instance(\n",
    "                        X_ts_sel[i],\n",
    "                        predict_fn_lime,\n",
    "                        num_features=min(10, X_ts_sel.shape[1]),\n",
    "                        labels=[pos_idx_local]\n",
    "                    )\n",
    "                    html = exp.as_html(labels=[pos_idx_local])\n",
    "                    with open(LIME_DIR / f\"{safe_model}__rep{rep:02d}__lime_idx{i}.html\",\n",
    "                              \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(html)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"[WARN] {model_name} rep={rep} LIME error: {e}\")\n",
    "\n",
    "            print(f\"[OK] {model_name} rep={rep}: SHAP/LIME saved with SELECTED features.\")\n",
    "\n",
    "print(\"\\nAll steps completed successfully.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12) Selected features + frequency + best hyperparameters (LONG/WIDE)\n",
    "# ============================================================\n",
    "SEL_DIR = BASE_OUT_DIR / f\"selected_features_{RUN_TAG}\"\n",
    "SEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _clean_names_local(names):\n",
    "    try:\n",
    "        return clean_names(names)\n",
    "    except NameError:\n",
    "        out = []\n",
    "        for n in names:\n",
    "            n = str(n)\n",
    "            out.append(n.split(\"__\", 1)[1] if \"__\" in n else n)\n",
    "        return out\n",
    "\n",
    "def get_feature_names_after_prep_only(pipe, X_ref):\n",
    "    pre = pipe.named_steps.get('prep', None)\n",
    "    if pre is not None and hasattr(pre, \"get_feature_names_out\"):\n",
    "        try:\n",
    "            names = list(pre.get_feature_names_out(input_features=X_ref.columns))\n",
    "        except Exception:\n",
    "            try:\n",
    "                names = list(pre.get_feature_names_out())\n",
    "            except Exception:\n",
    "                names = list(X_ref.columns)\n",
    "    else:\n",
    "        names = list(X_ref.columns)\n",
    "    return names, _clean_names_local(names)\n",
    "\n",
    "def safe_array(a, target_len):\n",
    "    if a is None:\n",
    "        return np.array([np.nan] * target_len, dtype=float)\n",
    "    a = np.asarray(a)\n",
    "    L = min(len(a), target_len)\n",
    "    return np.asarray(a[:L], dtype=float)\n",
    "\n",
    "# 1) Long table of selected features per run\n",
    "rows = []\n",
    "for model_name in model_names:\n",
    "    for rep, pipe in (final_pipes.get(model_name, {}) or {}).items():\n",
    "        names_trans, names_clean = get_feature_names_after_prep_only(pipe, X)\n",
    "        feat = pipe.named_steps.get(\"feat\", None)\n",
    "        if feat is not None and hasattr(feat, \"get_support\"):\n",
    "            mask   = feat.get_support()\n",
    "            scores = getattr(feat, \"scores_\", None)\n",
    "            pvals  = getattr(feat, \"pvalues_\", None)\n",
    "        else:\n",
    "            mask   = np.ones(len(names_trans), dtype=bool)\n",
    "            scores = None\n",
    "            pvals  = None\n",
    "\n",
    "        L = len(names_trans)\n",
    "        if len(mask) != L:\n",
    "            L = min(L, len(mask))\n",
    "            names_trans = names_trans[:L]\n",
    "            names_clean = names_clean[:L]\n",
    "            mask = mask[:L]\n",
    "        scores = safe_array(scores, L)\n",
    "        pvals  = safe_array(pvals,  L)\n",
    "\n",
    "        # score-based ranking (descending)\n",
    "        score_for_rank = np.nan_to_num(scores, nan=-np.inf)\n",
    "        order = np.argsort(-score_for_rank)\n",
    "        rank_map = {int(idx): int(r+1) for r, idx in enumerate(order)}\n",
    "        k_selected = int(np.sum(mask))\n",
    "\n",
    "        for i in range(L):\n",
    "            cname = names_clean[i]\n",
    "            idx_in_X = X.columns.get_loc(cname) if cname in X.columns else np.nan\n",
    "            rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"rep\": int(rep),\n",
    "                \"k_selected\": k_selected,\n",
    "                \"feat_pos_after_prep\": i,\n",
    "                \"feat_idx_in_X\": int(idx_in_X) if idx_in_X == idx_in_X else np.nan,\n",
    "                \"feat_name_clean\": cname,\n",
    "                \"feat_name_transformed\": names_trans[i],\n",
    "                \"selected\": bool(mask[i]),\n",
    "                \"score_f_classif\": float(scores[i]) if scores[i] == scores[i] else np.nan,\n",
    "                \"p_value\": float(pvals[i]) if pvals[i] == pvals[i] else np.nan,\n",
    "                \"rank_desc_score\": rank_map.get(i, np.nan),\n",
    "            })\n",
    "\n",
    "df_selected_long = (pd.DataFrame(rows)\n",
    "                    .sort_values([\"model\",\"rep\",\"selected\",\"rank_desc_score\"],\n",
    "                                 ascending=[True, True, False, True])\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "out_long = SEL_DIR / \"selected_features_per_run_long.csv\"\n",
    "df_selected_long.to_csv(out_long, index=False)\n",
    "print(f\"[OK] Selected-features (long): {out_long}\")\n",
    "\n",
    "# 2) Selection frequency summary per model\n",
    "freq_rows = []\n",
    "if not df_selected_long.empty:\n",
    "    for (model_name, feat), g in df_selected_long.groupby([\"model\", \"feat_name_clean\"]):\n",
    "        n_runs = g[\"rep\"].nunique()\n",
    "        n_sel  = int(g.loc[g[\"selected\"], \"rep\"].nunique())\n",
    "        freq_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"feature\": feat,\n",
    "            \"n_runs\": n_runs,\n",
    "            \"n_selected\": n_sel,\n",
    "            \"selection_rate\": (n_sel / n_runs) if n_runs > 0 else np.nan,\n",
    "            \"mean_score_selected\": g.loc[g[\"selected\"] & g[\"score_f_classif\"].notna(), \"score_f_classif\"].mean(),\n",
    "            \"median_rank_among_all\": g[\"rank_desc_score\"].median()\n",
    "        })\n",
    "\n",
    "df_freq = (pd.DataFrame(freq_rows)\n",
    "           .sort_values([\"model\",\"selection_rate\",\"mean_score_selected\"],\n",
    "                        ascending=[True, False, False])\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "out_freq = SEL_DIR / \"selection_frequency_by_model.csv\"\n",
    "df_freq.to_csv(out_freq, index=False)\n",
    "print(f\"[OK] Selection frequency: {out_freq}\")\n",
    "\n",
    "# Plot top-15 by selection rate per model (for GitHub visuals)\n",
    "PLOT_DIR = SEL_DIR / \"plots\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for m in model_names:\n",
    "    sub = df_freq[df_freq['model'] == m].copy()\n",
    "    if sub.empty: continue\n",
    "    sub = sub.sort_values('selection_rate', ascending=False).head(15)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=sub, x='selection_rate', y='feature')\n",
    "    plt.xlabel('Selection rate over 20 runs')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top-15 selected features — {m} — {RUN_TAG}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / f\"top_features_{m.replace(' ','_')}_{RUN_TAG}.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# 3) Best hyperparameters — LONG and WIDE CSVs\n",
    "def _flatten_dict(d, parent_key=\"\"):\n",
    "    items = []\n",
    "    if d is None:\n",
    "        return items\n",
    "    for k, v in d.items():\n",
    "        key = f\"{parent_key}.{k}\" if parent_key else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(_flatten_dict(v, key))\n",
    "        else:\n",
    "            items.append((key, v))\n",
    "    return items\n",
    "\n",
    "hp_long_rows = []\n",
    "hp_wide_rows = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    for rec in best_hyperparameters.get(model_name, []):\n",
    "        rep = rec.get(\"rep\", np.nan)\n",
    "        bp    = rec.get(\"best_params\", {}) or {}\n",
    "        bps   = rec.get(\"best_params_structured\", {}) or {}\n",
    "        kbest = rec.get(\"k_best\", None)\n",
    "        valf1 = rec.get(\"val_f1\", np.nan)\n",
    "        valap = rec.get(\"val_ap\", np.nan)\n",
    "        valau = rec.get(\"val_auc\", np.nan)\n",
    "        opttg = rec.get(\"optimize_for\", None)\n",
    "        sel_list = rec.get(\"selected_feature_names_clean\", None)\n",
    "\n",
    "        # LONG\n",
    "        meta = {\n",
    "            \"meta.val_f1\": valf1, \"meta.val_ap\": valap, \"meta.val_auc\": valau,\n",
    "            \"meta.k_best\": kbest, \"meta.optimize_for\": opttg,\n",
    "            \"meta.selected_features_joined\": \";\".join(map(str, sel_list)) if sel_list else None\n",
    "        }\n",
    "        for k, v in meta.items():\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": k, \"value\": v})\n",
    "        for k, v in sorted(_flatten_dict(bp)):\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": f\"best_params.{k}\", \"value\": v})\n",
    "        for k, v in sorted(_flatten_dict(bps)):\n",
    "            hp_long_rows.append({\"model\": model_name, \"rep\": rep, \"param\": f\"best_params_structured.{k}\", \"value\": v})\n",
    "\n",
    "        # WIDE\n",
    "        wide_row = {\"model\": model_name, \"rep\": rep,\n",
    "                    \"val_f1\": valf1, \"val_ap\": valap, \"val_auc\": valau,\n",
    "                    \"k_best\": kbest, \"optimize_for\": opttg,\n",
    "                    \"selected_features_joined\": \";\".join(map(str, sel_list)) if sel_list else None}\n",
    "        for k, v in _flatten_dict(bp, \"best_params\"):\n",
    "            wide_row[k] = v\n",
    "        for k, v in _flatten_dict(bps, \"best_params_structured\"):\n",
    "            wide_row[k] = v\n",
    "        hp_wide_rows.append(wide_row)\n",
    "\n",
    "df_hp_long = pd.DataFrame(hp_long_rows).sort_values([\"model\",\"rep\",\"param\"]).reset_index(drop=True)\n",
    "df_hp_wide = pd.DataFrame(hp_wide_rows).sort_values([\"model\",\"rep\"]).reset_index(drop=True)\n",
    "\n",
    "def _jsonify_non_scalars(df, col):\n",
    "    def _to_json_if_needed(x):\n",
    "        if isinstance(x, (list, tuple, dict)):\n",
    "            try:\n",
    "                return json.dumps(x, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                return str(x)\n",
    "        return x\n",
    "    df[col] = df[col].apply(_to_json_if_needed)\n",
    "\n",
    "if not df_hp_long.empty:\n",
    "    _jsonify_non_scalars(df_hp_long, \"value\")\n",
    "\n",
    "out_hp_long = SEL_DIR / \"best_hyperparameters_long.csv\"\n",
    "out_hp_wide = SEL_DIR / \"best_hyperparameters_wide.csv\"\n",
    "df_hp_long.to_csv(out_hp_long, index=False)\n",
    "df_hp_wide.to_csv(out_hp_wide, index=False)\n",
    "print(f\"[OK] Best hyperparameters (long): {out_hp_long}\")\n",
    "print(f\"[OK] Best hyperparameters (wide): {out_hp_wide}\")\n",
    "\n",
    "# Also persist the full dict as JSON (already saved earlier under OUT_DIR)\n",
    "bhp_json_out = SEL_DIR / \"best_hyperparameters_with_selected_features.json\"\n",
    "try:\n",
    "    with open(bhp_json_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_hyperparameters, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] JSON snapshot (bhp+selected): {bhp_json_out}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] JSON write skipped: {e}\")\n",
    "\n",
    "print(\"\\n[REPORT]\")\n",
    "print(f\"  - Selected features (long): {out_long}\")\n",
    "print(f\"  - Selection frequency      : {out_freq}\")\n",
    "print(f\"  - Best HP (long)           : {out_hp_long}\")\n",
    "print(f\"  - Best HP (wide)           : {out_hp_wide}\")\n",
    "print(f\"  - JSON snapshot            : {bhp_json_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
